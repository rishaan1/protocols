# Running Jupyter Lab on the SBP Pines Compute Cluster
Sometimes, it becomes the case where your desktop does not have enough computational resources (e.g. memory, disk space, etc) to run your Bioinformatics/computational tasks. 
Jupyter lab is my preferred environment of writing code, and it can also be accessed through the Pines cluster. Here are the steps to run it. 

There is a script in the file path `:/mnt/beegfs/shares/chavez_lab/expanse/scripts/`. You can see what the script contains using the following commands: 
```bash
ssh pines
#Enter your password
cd /mnt/beegfs/shares/chavez_lab/expanse/scripts/
cat jupyter.sh
```
Let's go line-by-line for the script:

```#!/bin/bash
# This line specifies the script interpreter to be Bash. No touchy!

#SBATCH --job-name=jupyter
# The name of the job as it will appear in the job queue. You can change it as you see fit. 

#SBATCH --nodes=1
# Request 1 node for the job. 

#SBATCH --ntasks=1
# Request 1 task for the job

#SBATCH --cpus-per-task=4
# Request 4 CPUs for this task 

#SBATCH --mem=16G
# Request 16 GB of memory for the job. Can change this to how much memory you need. 

#SBATCH --time=04:00:00
# Set a time limit for the job of 4 hours. Can change this to however long you need. 

#SBATCH --output=jupyter_%j.log
# Direct the output of the job to a file named jupyter_<job_id>.log, where <job_id> is replaced with the job ID

module load anaconda
# Load the Anaconda module to access the Python environment

jupyter-lab --no-browser --port=8893 --ip=0.0.0.0
# Start JupyterLab without opening a browser, set it to listen on port 8893 and on all IP addresses (0.0.0.0)
```

I would first recommend copying the file `jupyter.sh` to your directory where you want to run Jupyter, using `cp jupyter.sh /path/to/desination_dir`. Then you can use 'cd path/to/desination_dir`. 

Now, within your destination directory, run the script using `sbatch jupyter.sh`. After doing this, it will provide an output like this:`Submitted batch job 243407`.
You can view the logs of this output using `cat jupyter_jobid.log`. In this case, it would be `cat jupyter_243407.log`.

Next, use `Control + T` on your keyboard to create a new terminal tab. Then use the bash command `ssh -L <port_number>:localhost:8893 <username>@<pines_ip>`. 
In this case, it would be `ssh -L 8899:localhost:8893 username@pines`. Input your password and you should be able to access Jupyter lab by opening a web
browser on your local machine and navigating to:
`http://localhost:8899`

If JupyterLab asks for a password or token, it is typically provided in the log file generated by your SLURM job.
You can retrieve this information by checking the output log specified in your SLURM script. Here's how you can do it:
The output of your JupyterLab instance, including the URL with the token, will be written to the log file specified in your SLURM script (jupyter_%j.log).
The %j is replaced with the job ID. For example, if your job ID is 12345, the log file will be named jupyter_12345.log. You can find your job id by using `squeue -u username`
and navigating to the row where the job is. 

To check the content of the log file, use the cat command or any other method to read the file:
`cat jupyter_12345.log`. In my case I will use again `cat jupyter_243407.log`. 

After this, look for a line in the log file that contains the JupyterLab URL with a token, similar to this:
`http://localhost:8893/?token=your_token_here` 
When prompted for a password or token, enter the token you found in the log file. This should take you to the Jupyter lab environment where you can see the subdirectories and 
corresponding files in your destination directory. Lastly, it will disconnect if past the time limit. 


